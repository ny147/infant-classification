{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import shutil\n",
    "#import ftransc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 303 files belonging to 2 classes.\n",
      "Found 154 files belonging to 2 classes.\n",
      "['hungry', 'non_hungry']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_directory = './data/mel_freq/train_oneclass/hungry_one'\n",
    "test_directory = './data/mel_freq/test_oneclass/hungry_one'\n",
    "#train_directory = './img_data/mel_spectrogram/train'\n",
    "#test_directory = './img_data/mel_spectrogram/test'\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_directory, labels='inferred', label_mode='int', image_size=(256, 256), seed=321,\n",
    "    validation_split=None, subset=None)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_directory, labels='inferred', label_mode='int', image_size=(256, 256),\n",
    "    validation_split=None, subset=None)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "for images, labels in train_ds.unbatch().take(-1):\n",
    "    x_train.append(images.numpy())\n",
    "    y_train.append(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=[]\n",
    "y_test=[]\n",
    "for images, labels in test_ds.unbatch().take(-1):\n",
    "    x_test.append(images.numpy())\n",
    "    y_test.append(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "def create_weight(n_sample,n_class,n_class_sample):\n",
    "    weight = n_sample/(n_class*n_class_sample)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0:0, 1:0}\n",
    "class_count = np.array([254,49])\n",
    "for i in range(num_classes):\n",
    "    class_weights[i]=create_weight(303,num_classes,class_count[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " rescaling_6 (Rescaling)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " tf.cast_5 (TFOpLambda)      (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 8, 8, 2048)        23564800  \n",
      "                                                                 \n",
      " global_average_pooling2d_5   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                131136    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,696,001\n",
      "Trainable params: 131,201\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.ResNet50V2(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    include_top=False,\n",
    ") \n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top\n",
    "inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "scale_layer = tf.keras.layers.Rescaling(scale=1./255)\n",
    "x = scale_layer(inputs)\n",
    "\n",
    "x = tf.cast(x,tf.float32)\n",
    "\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" #model = tf.keras.Sequential([\\n            tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\\n            tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\\n            tf.keras.layers.MaxPooling2D((2,2)),\\n            tf.keras.layers.Dropout(0.2),\\n            tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),            \\n            tf.keras.layers.MaxPooling2D((2,2)),\\n            tf.keras.layers.Dropout(0.2),\\n            tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\\n            tf.keras.layers.MaxPooling2D((2,2)),\\n            tf.keras.layers.Flatten(),\\n            tf.keras.layers.Dense(64, activation='relu'),\\n            tf.keras.layers.Dense(1, activation='sigmoid')\\n        ])\\nmodel.summary() \""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' #model = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "            tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2,2)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),            \n",
    "            tf.keras.layers.MaxPooling2D((2,2)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2,2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "model.summary() '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet \n",
    "#inception v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()# modify weight in Adam\n",
    "metrics = ['accuracy']\n",
    "model.compile (optimizer = optimizer,loss =  loss_fn,metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.7825 - accuracy: 0.5991 - val_loss: 0.8098 - val_accuracy: 0.2527\n",
      "Epoch 2/30\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.7001 - accuracy: 0.2972 - val_loss: 0.6573 - val_accuracy: 0.5275\n",
      "Epoch 3/30\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.6402 - accuracy: 0.7028 - val_loss: 0.5672 - val_accuracy: 0.7363\n",
      "Epoch 4/30\n",
      "7/7 [==============================] - 18s 3s/step - loss: 0.5891 - accuracy: 0.6981 - val_loss: 0.6236 - val_accuracy: 0.6484\n",
      "Epoch 5/30\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.5850 - accuracy: 0.5613 - val_loss: 0.6089 - val_accuracy: 0.6044\n",
      "Epoch 6/30\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.5764 - accuracy: 0.7642 - val_loss: 0.5147 - val_accuracy: 0.7912\n",
      "Epoch 7/30\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.5285 - accuracy: 0.7406 - val_loss: 0.6009 - val_accuracy: 0.6374\n",
      "Epoch 8/30\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.5214 - accuracy: 0.6651 - val_loss: 0.5231 - val_accuracy: 0.7363\n",
      "Epoch 9/30\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.4919 - accuracy: 0.8349 - val_loss: 0.5102 - val_accuracy: 0.7802\n",
      "Epoch 10/30\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.4871 - accuracy: 0.7925 - val_loss: 0.6077 - val_accuracy: 0.6813\n",
      "Epoch 11/30\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.4687 - accuracy: 0.7500 - val_loss: 0.5167 - val_accuracy: 0.7253\n",
      "Epoch 12/30\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.4362 - accuracy: 0.8302 - val_loss: 0.5092 - val_accuracy: 0.7253\n",
      "Epoch 13/30\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.4233 - accuracy: 0.8632 - val_loss: 0.5114 - val_accuracy: 0.7143\n",
      "Epoch 14/30\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.4150 - accuracy: 0.8821 - val_loss: 0.5223 - val_accuracy: 0.7253\n",
      "Epoch 15/30\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.4076 - accuracy: 0.7689 - val_loss: 0.4783 - val_accuracy: 0.7363\n",
      "Epoch 16/30\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.3599 - accuracy: 0.8962 - val_loss: 0.4550 - val_accuracy: 0.7802\n",
      "Epoch 17/30\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.3411 - accuracy: 0.8774 - val_loss: 0.5182 - val_accuracy: 0.7143\n",
      "Epoch 18/30\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.3502 - accuracy: 0.8443 - val_loss: 0.4893 - val_accuracy: 0.7582\n",
      "Epoch 19/30\n",
      "7/7 [==============================] - 18s 3s/step - loss: 0.3231 - accuracy: 0.8726 - val_loss: 0.4481 - val_accuracy: 0.7912\n",
      "Epoch 20/30\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.2809 - accuracy: 0.9481 - val_loss: 0.4611 - val_accuracy: 0.7802\n",
      "Epoch 21/30\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.3000 - accuracy: 0.8774 - val_loss: 0.4395 - val_accuracy: 0.7912\n",
      "Epoch 22/30\n",
      "7/7 [==============================] - 19s 3s/step - loss: 0.2763 - accuracy: 0.9434 - val_loss: 0.4891 - val_accuracy: 0.7582\n",
      "Epoch 23/30\n",
      "7/7 [==============================] - 17s 3s/step - loss: 0.3013 - accuracy: 0.8349 - val_loss: 0.4208 - val_accuracy: 0.7802\n",
      "Epoch 24/30\n",
      "7/7 [==============================] - 17s 3s/step - loss: 0.2597 - accuracy: 0.9528 - val_loss: 0.4875 - val_accuracy: 0.7473\n",
      "Epoch 25/30\n",
      "7/7 [==============================] - 18s 3s/step - loss: 0.2373 - accuracy: 0.9104 - val_loss: 0.4133 - val_accuracy: 0.8132\n",
      "Epoch 26/30\n",
      "7/7 [==============================] - 18s 3s/step - loss: 0.2439 - accuracy: 0.9434 - val_loss: 0.4822 - val_accuracy: 0.7473\n",
      "Epoch 27/30\n",
      "7/7 [==============================] - 18s 3s/step - loss: 0.2222 - accuracy: 0.9434 - val_loss: 0.4456 - val_accuracy: 0.7802\n",
      "Epoch 28/30\n",
      "7/7 [==============================] - 18s 3s/step - loss: 0.2194 - accuracy: 0.9340 - val_loss: 0.4611 - val_accuracy: 0.7692\n",
      "Epoch 29/30\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.1967 - accuracy: 0.9528 - val_loss: 0.4233 - val_accuracy: 0.8022\n",
      "Epoch 30/30\n",
      "7/7 [==============================] - 13s 2s/step - loss: 0.2012 - accuracy: 0.9481 - val_loss: 0.4825 - val_accuracy: 0.7692\n"
     ]
    }
   ],
   "source": [
    "# Set the epocks\n",
    "# ทำ stop + validation\n",
    "epochs = 30\n",
    "history = model.fit(x_train, y_train, epochs=epochs,validation_split=0.3,class_weight = class_weights,callbacks=callback)\n",
    "\n",
    "#history = model.fit(x_train, y_train, epochs=epochs,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " rescaling_6 (Rescaling)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " tf.cast_5 (TFOpLambda)      (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 8, 8, 2048)        23564800  \n",
      "                                                                 \n",
      " global_average_pooling2d_5   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                131136    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,696,001\n",
      "Trainable params: 23,650,561\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "7/7 [==============================] - 50s 7s/step - loss: 2.0526 - binary_accuracy: 0.6651 - val_loss: 0.5902 - val_binary_accuracy: 0.7253\n",
      "Epoch 2/15\n",
      "7/7 [==============================] - 50s 7s/step - loss: 0.6721 - binary_accuracy: 0.6274 - val_loss: 0.5255 - val_binary_accuracy: 0.7692\n",
      "Epoch 3/15\n",
      "7/7 [==============================] - 60s 8s/step - loss: 0.6209 - binary_accuracy: 0.6698 - val_loss: 0.5528 - val_binary_accuracy: 0.7363\n",
      "Epoch 4/15\n",
      "7/7 [==============================] - 46s 7s/step - loss: 0.5452 - binary_accuracy: 0.6840 - val_loss: 0.4141 - val_binary_accuracy: 0.8352\n",
      "Epoch 5/15\n",
      "7/7 [==============================] - 49s 7s/step - loss: 0.5384 - binary_accuracy: 0.7406 - val_loss: 0.4301 - val_binary_accuracy: 0.8242\n",
      "Epoch 6/15\n",
      "7/7 [==============================] - 46s 7s/step - loss: 0.4633 - binary_accuracy: 0.8160 - val_loss: 0.3978 - val_binary_accuracy: 0.8352\n",
      "Epoch 7/15\n",
      "7/7 [==============================] - 46s 7s/step - loss: 0.2872 - binary_accuracy: 0.8868 - val_loss: 0.4533 - val_binary_accuracy: 0.7912\n",
      "Epoch 8/15\n",
      "7/7 [==============================] - 62s 9s/step - loss: 0.1933 - binary_accuracy: 0.9198 - val_loss: 0.5364 - val_binary_accuracy: 0.7363\n",
      "Epoch 9/15\n",
      "7/7 [==============================] - 50s 7s/step - loss: 0.2997 - binary_accuracy: 0.8821 - val_loss: 0.5770 - val_binary_accuracy: 0.6593\n",
      "Epoch 10/15\n",
      "7/7 [==============================] - 50s 7s/step - loss: 0.2857 - binary_accuracy: 0.8302 - val_loss: 0.4729 - val_binary_accuracy: 0.8352\n",
      "Epoch 11/15\n",
      "7/7 [==============================] - 55s 8s/step - loss: 0.4003 - binary_accuracy: 0.8491 - val_loss: 0.4286 - val_binary_accuracy: 0.7692\n"
     ]
    }
   ],
   "source": [
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(5e-5),  # Low learning rate\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 15\n",
    "history = model.fit(x_train, y_train, epochs=epochs,validation_split=0.3,class_weight = class_weights,callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_transform(pred):\n",
    "    if pred > 0.5:\n",
    "        predicted = 1\n",
    "    else:\n",
    "        predicted = 0\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "\n",
    "vfunc = np.vectorize(binary_transform)\n",
    "y_pred = vfunc(pred)\n",
    "actual = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEHCAYAAABlbhceAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAct0lEQVR4nO3de7xVdZ3/8dcbkCFCBUUREcUcTEvREBkzcxwly8ZSS3/WzzG8lL9sdDKttElzbpaa5m/UrEE08DpqijpWYpKXrIQfkBdQAS8gmKKA4BWVcz6/P9Y6sjwezl5nn7PP2mvv9/Px2I+z9nd/91qfzeWzv+ezvuu7FBGYmVk59Ck6ADMzy89J28ysRJy0zcxKxEnbzKxEnLTNzEqkX9EBNLKhm/WNUSM3KjoM64JF8wcVHYJ10SstK1dExBbd2cen/+6DsXJVS66+cx55a3pEfKY7x+sOJ+0aGjVyI2ZNH1l0GNYFn91p36JDsC6avubKJd3dx8pVLcyavm2uvn2HLxra3eN1h5O2mTW9AFppLTqMXJy0zazpBcE7ka88UjQnbTMzPNI2MyuNIGgpyZIeTtpmZkArTtpmZqUQQIuTtplZeXikbWZWEgG845q2mVk5BOHyiJlZaQS0lCNnO2mbmSVXRJaDk7aZGaIFFR1ELk7aZtb0Amh1ecTMrBwCeLsktxdw0jYzA1rD5REzs1JIroh00jYzK4VAtLg8YmZWHi6PmJmVRCDejr5Fh5GLk7aZNb3k4hqXR8zMSsMnIs3MSiJCtIRH2mZmpdFakpF2Ob5azMxqKJmn3SfXoxJJV0p6UdK8TNtmkn4raVH6c0jaLkkXS3pS0iOSxlbav5O2mTW9QLwT/XI9cpgCfKZd2xnAjIgYDcxInwMcBIxOHycAP6u0cydtMzOgJZTrUUlE3A+satd8CDA13Z4KHJppvyoSDwKDJQ3vbP+uaZtZ0+viFZFDJc3OPJ8UEZMqvGdYRDwPEBHPS9oybR8BLM30W5a2Pb+hHTlpm5kBrflnj6yIiHE9dNiOhu6dLhLrpG1mTa/tRGQNLZc0PB1lDwdeTNuXASMz/bYB/tLZjlzTNrOmF+SrZ+epaW/A7cDEdHsicFum/SvpLJK9gDVtZZQN8UjbzJpeBHlnhlQk6XpgP5La9zLgbOBc4EZJxwPPAkek3X8NfBZ4EngDOLbS/p20zcxQj11cExFf3sBLB3TQN4B/7Mr+nbTNrOkF+DJ2M7My8U0QzMxKIpBvgmBmVhZBz52IrLVyRGlmVlPyetpmZmURdOmKyEI5aZuZ4TvXmJmVRoQ80jYzKxPP0zYzK4nkJgh9iw4jFydtM2t6yYlI17TNzErDV0SamZWEr4g0MyuZVo+0zczKIYLu3OCgVzlpm1nTC8S6Vs8eMTMrDV8RaaV14bdGMvPuTRg8dB2T7lkAwP3/sylXX7gVSxcN4OJfL2TH3d4E4Ik/D+Q/v5PclzSAo097gU8ctKao0A045ZyFjN9vFatXbsQ3Pr8HAEedtIRPH/ECa1ZtBMDUi0Yx+/7NigyzrnjKXy+QNAq4IyJ2KTiUhnPgkav4/LEr+PE3t323bdROa/nB5MVcfPrI9/Qd9eE3ufTOBfTtByuX9+PECR9mr0+toW9p/2WV393ThvE/127NaecueE/7rVNHcMuV2xQUVb3zZewNR1K/iFhXdBy9Yde9XueFpf3f07bt6Lc67DtgYLy7/c5bfVA5BisNbd7sTdlyxNqiwyidnrpHZK2VPWn3lXQ5sDfwHHAI8Bvg2xExW9JQYHZEjJJ0DPB5YCCwAzAtIr4LkN4h+XTgL8Ai4K2IOEnSFGAV8DHgIUkHA3tHxEuS+gALgb0iYkXvfeT688TcgVx46kheXNaf717yrEfZdepzR/2FAw5ZzqJ5GzP5vO157ZWNig6pbkTAOyU5EVmO3wc2bDTw04j4KLAa+GKF/rsDRwK7AkdKGilpa+AsYC/gU8BO7d6zIzAhIr4FXAMclbZPAB5un7AlnSBptqTZL61s6cZHK4+dxr7B5fcu4JLfLOS/L9mSt9eWY8TSTH51/XCO/9SenHToWFa91J+vnv5M0SHVlbaLa/I8ilb2pP1MRDyUbs8BRlXoPyMi1kTEWuAxYDtgPHBfRKyKiHeAm9q956aIaMu+VwJfSbePA37R/gARMSkixkXEuC02L8c3d0/ZdvRbDBjYyuIFA4oOxdpZvbI/ra0iQtx501bsuOurRYdUd1pRrkfRyp60s4XWFpJyzzrWf6722aOj/pX+Fl5v24iIpcBySfsDf0NSimlqLzzbn5a00r982UYse2oAw7Z5u9ig7H2GbLH+72TvCStZsmhggdHUn7bZI2UYaTdi9XExsAcwCzg8R/9ZwEWShgCvkpRYHu2k/2SSMsnVmRF4Q/nRidvxyJ8GsWZVP47a4yMcfdoLbDykhcvOHMGalf046+gPscNH3+SH1z/NvFkf5IZLt6dfP+jTJzj5h8vYdPOG/GMpje9e+ARj9lzNJkPWcdW9M7nmku0YM34NH9r5NSJg+XMDuOTs0UWHWXc8e6Q4FwA3Sjoa+F2lzhHxnKQfAjNJTkQ+BnQ20fh2krLI+0ojjeJ7P1vSYXtH868nHP4yEw5/udYhWRecf1r70zJw181bFRBJidTJKDqP0ibtiFgM7JJ5fkHm5TGZ7TPT16cAUzL9D870uS4iJknqB0wD7kr7HNPBoXcjOQH5RLc+gJnVjQDWeaRdKv8iaQJJDfwu4NaOOkk6AziR9TNIzKwB+IrIkomIb+fsdy5wbo3DMbMCOGmbmZWEb4JgZlYy9TAHOw8nbTOzcHnEzKw0AljX6tkjZmalUKaadjm+WszMaixCuR6VSPqWpPmS5km6XtIASdtLmilpkaQbJPWvuKMNcNI2M6NnFoySNAL4J2BceoOWvsCXgPOAiyJiNPAycHy1cTppm1nTi+jRBaP6AR9Ir7AeCDwP7A/8Mn19KnBotbG6pm1mhmjJfyJyqKTZmeeTImISvLuW0QXAs8CbJFdYzwFWZ+58tQwYUW2kTtpmZpCrXp1aERHjOnohXS30EGB7khuz3AQc1NHhqokRnLTNzHpy7ZEJJDdneQlA0i0kt0McnLnP7DYkK4pWxTVtM7NI6tp5HhU8C+wlaaAkAQeQLPd8D+vX958I3FZtqE7aZmb0zOyRiJhJcsJxLsnNVPoAk0huHH6qpCeBzYErqo3T5REza3pBl2rane8r4mzg7HbNT5Pcj7bbnLTNzBAtreW4ItJJ28yMnhtp15qTtpk1veQko5O2mVlplGXBKCdtMzNyTeerC07aZtb0AtHq9bTNzMqjJANtJ20zM3wi0sysZEoy1N5g0pa0SWdvjIhXej4cM7NiNMJIez7Jd0/2k7Q9D2DbGsZlZtarSj97JCJG9mYgZmZFiYAoyeyRXFFK+pKkf063t5G0R23DMjPrXT20NGvNVUzaki4F/g44Om16A/h5LYMyM+t1kfNRsDyzR/aOiLGS/gwQEau6c/t3M7P6o4Y4EdnmHUl9SL9jJG0OtNY0KjOz3lYHo+g88tS0fwrcDGwh6V+BB4DzahqVmVlvSi+uyfMoWsWRdkRcJWkOyQ0rAY6IiHm1DcvMrJfVQULOI+8VkX2Bd0h+gSjHvBgzs65olPKIpO8D1wNbk9z6/TpJ36t1YGZmvaqBZo/8A7BHRLwBIOkcYA7wo1oGZmbWa4KGKo8sadevH8mdhc3MGkY9XDiTR2cLRl1E8v3zBjBf0vT0+YEkM0jMzBpHA9yNvW2GyHzgV5n2B2sXjplZMVT2kXZEXNGbgZiZFaZOTjLmUbGmLWkH4BzgI8CAtvaI2LGGcZmZ9SKV5kRknjnXU4BfkKyjfRBwI/DfNYzJzKz3lWTKX56kPTAipgNExFMRcSbJqn9mZo2jJEk7z5S/tyQJeErS14HngC1rG5aZWS8KGmL2SJtvAYOAfyKpbW8KHFfLoMzMelvpZ4+0iYiZ6earrL8RgplZYyl70pY0jU4+RkR8oSYRmZnZBnU20r6016JoUIvmD+KzO+1bdBjWBS2vvFJ0CFaQ0pdHImJGbwZiZlaoHpqnLWkwMBnYhaRacRywALgBGAUsBv5XRLxczf69NraZWZDcRDHPo7L/BO6MiJ2A3YDHgTOAGRExGpiRPq+Kk7aZGUl5JM+j031ImwD7AlcARMTbEbEaOASYmnabChxabZy5k7akv6r2IGZmdS//xTVDJc3OPE7I7OVDwEvALyT9WdJkSR8EhkXE8wDpz6qvdclz55rxkh4FFqXPd5N0SbUHNDOrS/mT9oqIGJd5TMrspR8wFvhZRHwMeJ1ulEI6kmekfTFwMLASICIexpexm1kDyVsayTHDZBmwLHN9yy9JkvhyScMB0p8vVhtrnqTdJyKWtGtrqfaAZmZ1qVX5Hp2IiBeApZI+nDYdADwG3A5MTNsmArdVG2aey9iXShoPhKS+wMnAwmoPaGZWj3pwnvbJwLWS+pPcmvFYkgHyjZKOB54Fjqh253mS9okkJZJtgeXA3WmbmVnj6KGkHREPAeM6eOmAnth/nrVHXgS+1BMHMzOrS/nq1XUhz51rLqeD76CIOKGD7mZm5dQoSZukHNJmAHAYsLQ24ZiZFaRRknZE3JB9Lulq4Lc1i8jMrAANUx7pwPbAdj0diJlZoRolaUt6mfUfpw+wih6+wsfMrFCNciIyvTfkbiT3hQRojYiSfDQzsy4oSWbr9IrINEFPi4iW9FGSj2Vm1kUluRt7nsvYZ0kaW/NIzMwKInps7ZGa6+wekf0iYh2wD/A1SU+RrFglkkG4E7mZNYYA5bvBQeE6q2nPIlmdqurFus3MSqMORtF5dJa0BRART/VSLGZmxWmApL2FpFM39GJE/KQG8ZiZFaIe6tV5dJa0+wKDSEfcZmYNrQGS9vMR8W+9FomZWVEa5ESkR9hm1jwaYKTdIwt2m5mVQelr2hGxqjcDMTMrVNmTtplZ06iTS9TzcNI2s6YnynMSz0nbzIzGmD1iZtY8XB4xMysRJ20zs5Kok2VX83DSNjMDj7TNzMrEJyLNzErE5REzs7LwxTVmZiXjpG1mVg5tN/YtAydtMzPwSNvMrDQC1FqOrO2kbWaGyyNmZuVSkqTdp+gAzMzqgSLfI9e+pL6S/izpjvT59pJmSlok6QZJ/auN00nbzAzWz9Wu9Mjnm8DjmefnARdFxGjgZeD4asN00jYzyznKzjPSlrQN8PfA5PS5gP2BX6ZdpgKHVhuqa9pm1vREl9YeGSppdub5pIiYlHn+f4HvAhunzzcHVkfEuvT5MmBEtbE6aZuZAUTu2seKiBjX0QuSDgZejIg5kvZra+7oaF0PMOGkbWZGj035+wTweUmfBQYAm5CMvAdL6peOtrcB/lLtAVzTtopOOWch1/3hQS67fc67bUedtISr7pvJJdPmcsm0uYzbd1WBEVrWqT95lhsemc9//W7Bu22fPHg1k+55gt8se5jRY94oMLo6lfckZIXEHhHfi4htImIU8CXgdxFxFHAPcHjabSJwW7WhOmlbRXdPG8ZZX9vlfe23Th3ByYeN5eTDxjL7/s0KiMw6ctcNm/H9o7Z/T9viJwbwb18dxaMPfrCgqOqfWvM9qnQ6cKqkJ0lq3FdUu6NSlkckHQOMi4iTio6lGcybvSlbjlhbdBiW07yZgxi2zdvvaVv65ICCoimPnr4JQkTcC9ybbj8NjO+J/ZYyaRdBUt+IaCk6jnryuaP+wgGHLGfRvI2ZfN72vPbKRkWHZFadoCsnIgtVs/KIpFGSHpd0uaT5ku6S9AFJu0t6UNIjkqZJGpL2v1fSeZJmSVoo6ZMVDrG1pDvTK4zOzxz3tcz24ZKmpNtTJF0s6Y+SnpZ0eNreR9JlaYx3SPp15rXFkn4g6QHgDElzM/seLWl9kbfJ/Or64Rz/qT056dCxrHqpP189/ZmiQzLrlp68IrKWal3THg38NCI+CqwGvghcBZweEWOAR4GzM/37RcR44JR27R3ZHTgS2BU4UtLIHPEMB/YBDgbOTdu+AIxK9/NV4OPt3rM2IvaJiHOANZJ2T9uPBaa0P4CkEyTNljT77dbGLSmsXtmf1lYRIe68aSt23PXVokMy656evSKyZmqdtJ+JiIfS7TnADsDgiLgvbZsK7Jvpf0um76gK+54REWsiYi3wGLBdjnhujYjWiHgMGJa27QPclLa/QHKWN+uGzPZk4FhJfUm+MK5rf4CImBQR4yJiXP8+jVtHHLLF+prp3hNWsmTRwAKjMeuetpsglGGkXeua9luZ7RZgcM7+LVSOrf2+2/pn/1jbZ83se9Tu54a8ntm+meQ3gN8BcyJiZYX3NoTvXvgEY/ZczSZD1nHVvTO55pLtGDN+DR/a+TUiYPlzA7jk7NFFh2mpMy5bwpiPv8amm63jmtmPcfWFw3j15X584z+eY9PN1/HvVz/DU/MH8P3/vUPRodaPiNLUtHv7ROQa4GVJn4yI3wNHA/dVeE9XLZe0M7AAOAyo9Hv7A8BESVOBLYD96GAEDRARayVNB35GNxZ8KZvzT9vpfW133bxVAZFYHud+o+NfOv9456a9HEm59PTskVopYvbIRODnkgYCT5PUhnvSGcAdwFJgHjCoQv+bgQPSvguBmSRfLhtyLUkd/K5uR2pmdaMeSh951CxpR8RiYJfM8wsyL+/VQf/9Mtsr6KSmHRFTyJwEjIiDM9u/ZP1qWtn3HNPu+aD0Z6ukb0fEa5I2B2aRnCAlvaqpvX2AKz39z6yBBODbjZXKHZIGA/2Bf09PSL6PpGkkJ1P3783gzKwXlCNn13fSlvRpksXDs56JiMN68jjZUX6Ffj16XDOrH01fHukJETEdmF50HGbWBDx7xMysJMKzR8zMSiO5uMYjbTOz8vBI28ysPDzSNjMrizpZDCoPJ20zMwL54hozsxJxecTMrCQ85c/MrGQ80jYzK5Fy5GwnbTMz8JQ/M7PyCKDFSdvMrBREeKRtZlYqTtpmZiXipG1mVhKBF4wyMysT17TNzEojoLUcQ20nbTOzwDVtM7NSKcdA20nbzAxc0zYzK5eSJO0+RQdgZla4CGhpzffohKSRku6R9Lik+ZK+mbZvJum3khalP4dUG6qTtpkZJIk7z6Nz64DTImJnYC/gHyV9BDgDmBERo4EZ6fOqOGmbmUGPJO2IeD4i5qbbrwKPAyOAQ4CpabepwKHVhumatplZAD18j0hJo4CPATOBYRHxPCSJXdKW1e7XSdvMjIDIPedvqKTZmeeTImJStoOkQcDNwCkR8YqkHorTSdvMLJF/9siKiBi3oRclbUSSsK+NiFvS5uWShqej7OHAi9WG6Zq2mVnQU7NHBFwBPB4RP8m8dDswMd2eCNxWbageaZuZQU/N0/4EcDTwqKSH0rZ/Bs4FbpR0PPAscES1B3DSNjMj13S+ynuJeADYUAH7gG4fACdtM7N09kg5Fh9x0jYzg9Jcxu6kbWYGTtpmZqURQbS0FB1FLk7aZmbQ41dE1oqTtpkZuDxiZlYa4XtEmpmVi0faZmZl4RORZmblUYOlWWvFSdvMDLqyNGuhnLTNrOkFEB5pm5mVRHTpJgiFctI2M6M8I21FSaa5lJGkl4AlRcdRI0OBFUUHYV3SqH9n20XEFt3ZgaQ7Sf588lgREZ/pzvG6w0nbqiJpdme3XLL647+zxuDbjZmZlYiTtplZiThpW7UmFR2AdZn/zhqAa9pmZiXikbaZWYk4aZuZlYiTdhOSNErSvKLjMLOuc9K2mpPkK297kaRjJF1adBxWG07azauvpMslzZd0l6QPSLpX0jgASUMlLU63j5F0i6Q7JS2SdH7bTiQdL2lh+t7L25KFpCmSfiLpHuDH6fu2SF/rI+lJSXmvQLM6J6lv0TE0Cyft5jUa+GlEfBRYDXyxQv/dgSOBXYEjJY2UtDVwFrAX8Clgp3bv2RGYEBHfAq4BjkrbJwAPR0QjXlKdS1qieryDL87dJT0o6RFJ0yQNSfvfK+k8SbPSL8lPVjjE1hv4kn0ts324pCnp9hRJF0v6o6SnJR2etveRdFka4x2Sfp15bbGkH0h6ADhD0tzMvkdLmtNjf2D2Lift5vVMRDyUbs8BRlXoPyMi1kTEWuAxYDtgPHBfRKyKiHeAm9q956aIaLsdyJXAV9Lt44BfdPcDNICOvjivAk6PiDHAo8DZmf79ImI8cEq79o6870s2RzzDgX2Ag4Fz07YvkPzb2BX4KvDxdu9ZGxH7RMQ5wBpJu6ftxwJTchzTushJu3m9ldluIVnxcR3r/00MyNFfFY7xettGRCwFlkvaH/gb4DdVxNxo2n9x7gAMjoj70rapwL6Z/rdk+o6qsO+OvmQruTUiWiPiMWBY2rYPyZdva0S8ANzT7j03ZLYnA8empZIjgetyHNO6yEnbshYDe6Tbh+foPwv4W0lD0pONlUosk0nKJDdmRuDNrP0X4eCc/du+NLuy77b+2avpOvtiVrufG/J6Zvtm4CCSkfqciFhZ4b1WBSdty7oAOFHSH8mxTGVEPAf8EJgJ3E0yolvTyVtuBwbh0siGrAFeztSrjwbu66R/NZZL2llSH+CwHP0fAL6Y1raHAfttqGM6qp8O/Az/HdeMp2I1oYhYDOySeX5B5uUxme0z09enkKlPRsTBmT7XRcSkdKQ9Dbgr7XNMB4fejeQE5BPd+gCNbSLwc0kDgadJasM96QzgDmApMI/kS7QzNwMHpH0XknxBd/bFfC1JHfyubkdqHfLaI9Ytki4gmQ0ygOQ/6jejg39Uks4ATgSOiogHejdK6w5JgyLiNUmbk5TEPpHWtzvq+21g04g4q1eDbCJO2mbWKUn3ktTb+wPnp795ddRvGsnJ1P2beTpnrTlpm5WUpE8D57VrfiYi8tSqraSctM3MSsSzR8zMSsRJ28ysRJy0rVCSWiQ9JGmepJvSqW7V7ms/SXek259PZ6xsqO9gSd+o4hj/ks6QyNXers+UtnU7ch7LS+ja+zhpW9HejIjdI2IX4G3g69kXlejyv9OIuD0izu2ky2Cgy0nbrGhO2lZPfg/8dWYFvMuAucBISQdK+pOkuemIfBCApM9IeiJdae4LbTvKriktaVi6Yt7D6WNvkgWRdkhH+T9O+31H0v9LV9j718y+vi9pgaS7gQ9X+hCSvpbu52FJN7f77WGCpN+nK/UdnPbvK+nHmWP/n+7+QVrjctK2upBeUXkQycp2kCTHqyLiYyTrW5xJsszrWGA2cKqkAcDlwOeATwJbbWD3F5OsRrgbMBaYT3Jl4FPpKP87kg4kWXVvPMkKeXtI2lfSHsCXgI+RfCnsmePj3BIRe6bHexw4PvPaKOBvgb8nufJxQPr6mojYM93/1yRtn+M41oR8GbsV7QOS2la6+z1wBbA1sCQiHkzb9wI+AvxBEiQXefyJZP3uZyJiEYCka4ATOjjG/qTLwqYLVa1pW6c648D08ef0+SCSJL4xMC0i3kiPcXuOz7SLpP8gKcEMIlmPo82NEdEKLJL0dPoZDgTGZOrdm6bHXpjjWNZknLStaG9GxO7ZhjQxZ1ePE/DbiPhyu367895V67pDwI8i4r/aHeOUKo4xBTg0Ih6WdAzvXWSp/b4iPfbJEZFN7kga1cXjWhNwecTK4EHgE5L+GkDSQEk7Ak8A20vaIe335Q28fwbJuidt9eNNgFdJRtFtpgPHZWrlIyRtCdwPHKbkrjIbk5RiKtkYeF7SRqy/W0+bI9IV83YAPgQsSI99YtofSTtK+mCO41gT8kjb6l5EvJSOWK+X9Fdp85kRsVDSCcCvJK0gWUZ0lw528U1gkqTjSdaWPjEi/iTpD+mUut+kde2dgT+lI/3XgH+IiLmSbgAeApaQlHAqOYtkNbwlJDX67JfDApLlVocBX4+ItZImk9S65yo5+EvAofn+dKzZ+DJ2M7MScXnEzKxEnLTNzErESdvMrESctM3MSsRJ28ysRJy0zcxKxEnbzKxE/j9ozvsI/1nMiQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=class_names)\n",
    "disp.plot()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Hungry       0.88      0.88      0.88       128\n",
      "  Non_hungry       0.42      0.42      0.42        26\n",
      "\n",
      "    accuracy                           0.81       154\n",
      "   macro avg       0.65      0.65      0.65       154\n",
      "weighted avg       0.81      0.81      0.81       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred,target_names=[\"Hungry\",\"Non_hungry\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23bf0724a01b6ea9814e66f76182ea78c0ee849a72ca257c0e116bf83bb4960a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
